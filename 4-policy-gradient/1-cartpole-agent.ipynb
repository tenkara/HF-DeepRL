{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce (also called Monte Carlo Policy Gradient).\n",
    "\n",
    "Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm that tries **to optimize the policy directly without using an action-value function**.\n",
    "\n",
    "More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based methods* that aims **to optimize the policy directly by estimating the weights of the optimal policy using gradient ascent**.\n",
    "\n",
    "To test its robustness, we're going to train it in 2 different simple environments:\n",
    "- Cartpole-v1\n",
    "- PixelcopterEnv\n",
    "\n",
    "‚¨áÔ∏è Here is an example of what **you will achieve at the end of this notebook.** ‚¨áÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Environments\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Environments: \n",
    "\n",
    "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
    "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "### üìö RL-Library: \n",
    "\n",
    "- Python\n",
    "- PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives of this notebook üèÜ\n",
    "At the end of the notebook, you will:\n",
    "- Be able to **code from scratch a Reinforce algorithm using PyTorch.**\n",
    "- Be able to **test the robustness of your agent using simple environments.**\n",
    "- Be able to **push your trained agent to the Hub** with a nice video replay and an evaluation score üî•."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a virtual display üñ•\n",
    "\n",
    "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames). \n",
    "\n",
    "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! sudo apt install python3-opengl\n",
    "! sudo apt install ffmpeg\n",
    "! sudo apt install xvfb\n",
    "! pip install pyvirtualdisplay\n",
    "! pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f2b79b147f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies üîΩ\n",
    "The first step is to install the dependencies. We‚Äôll install multiple ones:\n",
    "\n",
    "- `gym`\n",
    "- `gym-games`: Extra gym environments made with PyGame.\n",
    "- `huggingface_hub`: ü§ó works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations, and other features that will allow you to easily collaborate with others.\n",
    "\n",
    "You may be wondering why we install gym and not gymnasium, a more recent version of gym? **Because the gym-games we are using are not updated yet with gymnasium**. \n",
    "\n",
    "The differences you'll encounter here:\n",
    "- In `gym` we don't have `terminated` and `truncated` but only `done`.\n",
    "- In `gym` using `env.step()` returns `state, reward, done, info`\n",
    "\n",
    "You can learn more about the differences between Gym and Gymnasium here üëâ https://gymnasium.farama.org/content/migration-guide/\n",
    "\n",
    "\n",
    "You can see here all the Reinforce models available üëâ https://huggingface.co/models?other=reinforce\n",
    "\n",
    "And you can find all the Deep Reinforcement Learning models here üëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
      "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-eumi11ob\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-eumi11ob\n",
      "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
      "  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-hzbab4ne\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-hzbab4ne\n",
      "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: pyyaml==6.0 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: Pillow in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (10.0.0)\n",
      "Requirement already satisfied: numpy in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.25.0)\n",
      "Requirement already satisfied: gym>=0.13.0 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\n",
      "Requirement already satisfied: pygame>=1.9.6 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.1.3)\n",
      "Collecting setuptools>=65.5.1\n",
      "  Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "Requirement already satisfied: filelock in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: fsspec in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.7.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: requests in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raj/repos/HF-DeepRL/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.5.7)\n",
      "Building wheels for collected packages: ple, gym-games\n",
      "  Building wheel for ple (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50785 sha256=19e57faad501d25e479dca29769a2c407467395b759ed872dff5ecbd75682489\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tl3gxa38/wheels/f8/31/ca/a64a7ce73540465412d82813780d062db53b90e3f42a4ecb7f\n",
      "  Building wheel for gym-games (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=17320 sha256=02e8113e0168e9aa26a1d1b3726306a611da4ca055faa35c779c69f1dd235f98\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tl3gxa38/wheels/ca/bf/6b/7d631626202ebb033c908a688d1862ff4d948c34cf621d7dc9\n",
      "Successfully built ple gym-games\n",
      "Installing collected packages: setuptools, ple, gym-games\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 59.6.0\n",
      "    Uninstalling setuptools-59.6.0:\n",
      "      Successfully uninstalled setuptools-59.6.0\n",
      "Successfully installed gym-games-1.0.4 ple-0.0.1 setuptools-68.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages üì¶\n",
    "In addition to import the installed libraries, we also import:\n",
    "\n",
    "- `imageio`: A library that will help us to generate a replay video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have a GPU\n",
    "\n",
    "- Let's check if we have a GPU\n",
    "- If it's the case you should see `device:cuda0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First agent: Playing CartPole-v1 ü§ñ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we use a simple environment like CartPole-v1?\n",
    "As explained in [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html), when you implement your agent from scratch you need **to be sure that it works correctly and find bugs with easy environments before going deeper**. Since finding bugs will be much easier in simple environments.\n",
    "\n",
    "\n",
    "> Try to have some ‚Äúsign of life‚Äù on toy problems\n",
    "\n",
    "\n",
    "> Validate the implementation by making it run on harder and harder envs (you can compare results against the RL zoo). You usually need to run hyperparameter optimization for that step.\n",
    "___\n",
    "#### The CartPole-v1 environment\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "\n",
    "\n",
    "So, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n",
    "\n",
    "The episode ends if:\n",
    "- The pole Angle is greater than ¬±12¬∞\n",
    "- Cart Position is greater than ¬±2.4\n",
    "- Episode length is greater than 500\n",
    "\n",
    "We get a reward üí∞ of +1 every timestep the Pole stays in the equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the environment\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation environment\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___Observation space___\n",
      "The state space is:  4\n",
      "Sample observation:  [-2.8930407e+00 -1.1181414e+37  6.0141031e-02 -2.3420465e+38]\n"
     ]
    }
   ],
   "source": [
    "# Print observation space\n",
    "print(\"___Observation space___\")\n",
    "print(\"The state space is: \", s_size)\n",
    "print(\"Sample observation: \", env.observation_space.sample())   # Get a random observation from this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build the Reinforce Architecture\n",
    "This implementation is based on two implementations:\n",
    "- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
    "- [Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # create 2 fully connected layers\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        # state goes to fc1 and then we apply ReLU activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # output of fc1 goes to fc2 and then we apply softmax activation function\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take an action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample() # sample an action from the distribution, this is a tensor\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Reinforce Training Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Reinforce algorithm pseudocode:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we calculate the return Gt (line 6) we see that we calculate the sum of discounted rewards **starting at timestep t**.\n",
    "\n",
    "- Why? Because our policy should only **reinforce actions on the basis of the consequences**: so rewards obtained before taking an action are useless (since they were not because of the action), **only the ones that come after the action matters**.\n",
    "\n",
    "- Before coding this you should read this section [don't let the past distract you](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) that explains why we use reward-to-go policy gradient.\n",
    "\n",
    "We use an interesting technique coded by [Chris1nexus](https://github.com/Chris1nexus) to **compute the return at each timestep efficiently**. The comments explained the procedure. Don't hesitate also [to check the PR explanation](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "But overall the idea is to **compute the return at each timestep efficiently**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second question you may ask is **why do we minimize the loss**? You talked about Gradient Ascent not Gradient Descent?\n",
    "\n",
    "- We want to maximize our utility function $J(\\theta)$ but in PyTorch like in Tensorflow it's better to **minimize an objective function.**\n",
    "    - So let's say we want to reinforce action 3 at a certain timestep. Before training this action P is 0.25.\n",
    "    - So we want to modify $\\theta$ such that $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - Because all P must sum to 1, max $\\pi_\\theta(a_3|s; \\theta)$ will **minimize other action probability.**\n",
    "    - So we should tell PyTorch **to min $1 - \\pi_\\theta(a_3|s; \\theta)$.**\n",
    "    - This loss function approaches 0 as $\\pi_\\theta(a_3|s; \\theta)$ nears 1.\n",
    "    - So we are encouraging the gradient to max $\\pi_\\theta(a_3|s; \\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to colculate the score during training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # line 3 of psuedocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset() # reset the environment\n",
    "        # line 4 of psuedocode: for t = 1, T do\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state) # get an action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) # take action in the environment\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # line 6 of psuedocode: calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0) \n",
    "            returns.appendleft(rewards[t] + gamma * disc_return_t) # calculate the return\n",
    "\n",
    "        # standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        # eps is the smallest representable float, which is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # line 7 of psuedocode: update policy parameters\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # line 8 of psuedocode: update policy parameters\n",
    "        # pytorch prefers gradient descent over gradient ascent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print the score every [100] episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train it\n",
    "- We're now ready to train our agent.\n",
    "- But first, we define a variable containing all the training hyperparameters.\n",
    "- You can change the training parameters (and should üòâ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluating_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "    \"print_every\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scores \u001b[39m=\u001b[39m reinforce(\n\u001b[1;32m      3\u001b[0m     cartpole_policy,\n\u001b[1;32m      4\u001b[0m     cartpole_optimizer,\n\u001b[1;32m      5\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      7\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mprint_every\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mreinforce\u001b[0;34m(policy, optimizer, n_training_episodes, max_t, gamma, print_every)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# line 4 of psuedocode: for t = 1, T do\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_t):\n\u001b[0;32m---> 12\u001b[0m     action, log_prob \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mact(state) \u001b[39m# get an action\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     saved_log_probs\u001b[39m.\u001b[39mappend(log_prob)\n\u001b[1;32m     14\u001b[0m     state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action) \u001b[39m# take action in the environment\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mPolicy.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Given a state, take an action\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(state)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     23\u001b[0m     m \u001b[39m=\u001b[39m Categorical(probs)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "scores = reinforce(\n",
    "    cartpole_policy,\n",
    "    cartpole_optimizer,\n",
    "    cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "    cartpole_hyperparameters[\"max_t\"],\n",
    "    cartpole_hyperparameters[\"gamma\"],\n",
    "    cartpole_hyperparameters[\"print_every\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
