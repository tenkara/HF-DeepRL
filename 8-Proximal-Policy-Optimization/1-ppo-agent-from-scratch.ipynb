{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (628573827.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    <video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Here is an example of what we are trying to acheive:\n",
    "# %%html\n",
    "# <video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==65.5.0\n",
      "  Using cached setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools\n",
      "Successfully installed setuptools-65.5.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install setuptools==65.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already installed \n",
    "%%capture\n",
    "# !apt install python-opengl\n",
    "# !apt install ffmpeg\n",
    "# !apt install xvfb\n",
    "# !apt install swig cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5 in ./.venv/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: future in ./.venv/lib/python3.10/site-packages (from pyglet==1.5) (0.18.3)\n",
      "Requirement already satisfied: pyvirtualdisplay in ./.venv/lib/python3.10/site-packages (3.0)\n"
     ]
    }
   ],
   "source": [
    "# install virtual display dependencies\n",
    "# !pip install pyglet==1.5\n",
    "# !pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f35f4c82500>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.22 in ./.venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.10/site-packages (from gym==0.22) (1.25.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.10/site-packages (from gym==0.22) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./.venv/lib/python3.10/site-packages (from gym==0.22) (0.0.8)\n",
      "Requirement already satisfied: imageio-ffmpeg in ./.venv/lib/python3.10/site-packages (0.4.8)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n",
      "Requirement already satisfied: gym[box2d]==0.22 in ./.venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./.venv/lib/python3.10/site-packages (from gym[box2d]==0.22) (1.25.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.10/site-packages (from gym[box2d]==0.22) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./.venv/lib/python3.10/site-packages (from gym[box2d]==0.22) (0.0.8)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in ./.venv/lib/python3.10/site-packages (from gym[box2d]==0.22) (2.3.5)\n",
      "Requirement already satisfied: pygame==2.1.0 in ./.venv/lib/python3.10/site-packages (from gym[box2d]==0.22) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install project dependencies\n",
    "!pip install gym==0.22\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install huggingface_hub\n",
    "!pip install gym[box2d]==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the core implementation of PPO we're going to use the excellent [Costa Huang](https://costa.sh/) tutorial.\n",
    "- In addition to the tutorial, to go deeper you can read the 37 core implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "üëâ The video tutorial: https://youtu.be/MEt6rrxH8W4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Hugging Face integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dependencies we need to push our model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.1.4-py3-none-any.whl (98 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-1.27.1-py2.py3-none-any.whl (211 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Collecting pathtools (from wandb)\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in ./.venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=be176beb22b9e07c3d16a689ce691462799e9a2a9d3523c162f5f657a139a465\n",
      "  Stored in directory: /home/raj/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.4 GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 protobuf-4.23.4 sentry-sdk-1.27.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n"
     ]
    }
   ],
   "source": [
    "# Install Weights and Biases package as that is an optional dependency passed in as a flag to the training script through the main function\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (40.8.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "Successfully installed setuptools-68.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install the latest version of setuptools to avoid the error above installing wandb\n",
    "! pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define make_env function needed to setup the environment in the main function\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.Monitor(env, f\"videos/{run_name}\")\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return thunk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Agent class that will be called from the main function\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Using cached grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Using cached google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorboard) (1.25.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./.venv/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.10/site-packages (from tensorboard) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in ./.venv/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: werkzeug, urllib3, tensorboard-data-server, pyasn1, oauthlib, markdown, grpcio, cachetools, absl-py, rsa, pyasn1-modules, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.1 google-auth-2.21.0 google-auth-oauthlib-1.0.0 grpcio-1.56.0 markdown-3.4.3 oauthlib-3.2.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 urllib3-1.26.16 werkzeug-2.3.6\n"
     ]
    }
   ],
   "source": [
    "# install tensorboard\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new argument in parse_args() function to define the repo-id where we want to push the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==40.8.0\n",
      "  Using cached setuptools-40.8.0-py2.py3-none-any.whl (575 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "Successfully installed setuptools-40.8.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install setuptools==40.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.venv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.venv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.venv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
      "Requirement already satisfied: lit in ./.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.25.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup cuda device\n",
    "# check if gpu is available\n",
    "\"\"\"check if gpu is available\"\"\"\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define _evaluate_agent function for Step 3 of the package_to_hub function\n",
    "import numpy as np\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\" Evaluate the agent for ``n_eval_episodes`` episodes and returns the mean and std of the rewards.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: The number of episodes to evaluate the agent\n",
    "    :param policy: The policy to use to act by the agent\n",
    "    \"\"\"\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.cpu().numpy())\n",
    "            total_rewards_ep += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "            episode_rewards.append(total_rewards_ep)\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define record_video() function for Step 4 of the package_to_hub function\n",
    "import torch\n",
    "import imageio_ffmpeg\n",
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    images.append(img)\n",
    "    while not done:\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        # take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        images.append(img)\n",
    "    imageio_ffmpeg.imwrite(os.path.join(out_directory, \"preview.jpg\"), [np.array(img) for i, img in enumerate(images)], fps=fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function _generate_model_card definition for Step 5 of the package_to_hub function\n",
    "\n",
    "# First define the dependent generate metadata function which is needed for the _generate_model_card\n",
    "def generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "  \"\"\"\n",
    "  Generate the metadata for the model card\n",
    "  :param model_name: name of the model\n",
    "  :env_id: name of the environment\n",
    "  :mean_reward: mean reward of the agent\n",
    "  :std_reward: standard deviation of the mean reward of the agent\n",
    "  \"\"\"\n",
    "  metadata = {}\n",
    "  metadata[\"tags\"] = [\n",
    "    env_id,\n",
    "    \"ppo\",\n",
    "    \"deep-reinforcement-learning\",\n",
    "    \"reinforcement-learning\",\n",
    "    \"custom-implementation\",\n",
    "    \"deep-rl-course\"\n",
    "  ]\n",
    "\n",
    "  # Add metrics\n",
    "  eval = metadata_eval_result(\n",
    "    model_pretty_name=model_name,\n",
    "    task_pretty_name=\"reinfrocement-learning\",\n",
    "    task_id=\"reinforcement-learning\",\n",
    "    metrics_pretty_name=\"mean_reward\",\n",
    "    metrics_id=\"mean_reward\",\n",
    "    metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "    dataset_pretty_name=env_id,\n",
    "    dataset_id=env_id,\n",
    "  )\n",
    "\n",
    "  # Merges both dictionaries\n",
    "  metadata = {**metadata, **eval}\n",
    "\n",
    "  return metadata\n",
    "      \n",
    "\n",
    "# generate model card\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "  \"\"\"\n",
    "  Generate the model card for the Hub\n",
    "  :param model_name: name of the model\n",
    "  :env_id: name of the environment\n",
    "  :mean_reward: mean reward of the agent\n",
    "  :std_reward: standard deviation of the mean reward of the agent\n",
    "  :hyperparameters: training arguments\n",
    "  \"\"\"\n",
    "  # Step 1: Select the tags\n",
    "  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "  # Transform the hyperparameters namespace into a string\n",
    "  converted_dict = vars(hyperparameters)\n",
    "  converted_str = str(converted_dict)\n",
    "  converted_str = converted_str.split(\", \")\n",
    "  converted_str = \"\\n\".join(converted_str)\n",
    "\n",
    "  # Step 2: Generate the model card\n",
    "  model_card = f\"\"\"\n",
    "  # PPO Agent Playing {env_id}\n",
    "\n",
    "  This is a trained model of a PPO agent playing {env_id}.\n",
    "\n",
    "  # Hyperparameters\n",
    "  ```python\n",
    "  {converted_str}\n",
    "  ```\n",
    "  \"\"\"\n",
    "  return model_card, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "# Demo:\n",
      "```python\n",
      "Hello world!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# understanding nested triple quotes\n",
    "str = \"Hello world!\"\n",
    "card = f\"\"\" \n",
    "# Demo:\n",
    "```python\n",
    "{str}\n",
    "```\n",
    "\"\"\"\n",
    "print(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function save_model_card needed for step 5 of the package_to_hub function\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"\n",
    "    Saves a model card to the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param generated_model_card: generated model card by _generate_model_card()\n",
    "    :param metadata: generated metadata\n",
    "    \"\"\"\n",
    "\n",
    "    readme_path = local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save the metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function _add_logdir needed for step 6 of the package_to_hub function\n",
    "def _add_logdir(local_path: Path, logdir: Path):\n",
    "    \"\"\"\n",
    "    Adds the logdir to the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param logdir: logdir to add\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # Add the logdir to the repository under new dir called logs\n",
    "        repo_logdir = local_path / \"logs\"\n",
    "\n",
    "        # Delete current logs if they exist\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # Copy logdir into repo logdir\n",
    "        shutil.copytree(logdir, repo_logdir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from wasabi import msg\n",
    "from pathlib import Path\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, upload_folder\n",
    "from typing import Optional\n",
    "import tempfile\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   commit_message=\"Push agent to the hub\",\n",
    "                   token=None,\n",
    "                   logs=None\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the hub\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param model: trained model\n",
    "    :param eval_env: environment used to evaluate the agent\n",
    "    :param fps: number of fps for rendering the video\n",
    "    :param commit_message: commit message\n",
    "    :param logs: directory on local machine of tensorboard logs you'd like to upload   \n",
    "    \"\"\"\n",
    "\n",
    "    msg.info(\n",
    "        \"This function will save, evaluate, generate a video of your agent, \"\n",
    "        \"create a model card and push everything to the hub. \"\n",
    "        \"It might take up to 1min. \\n \"\n",
    "        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n",
    "    )\n",
    "\n",
    "    # Step 1: Clone or create the repo\n",
    "    repo_url = HfApi().create_repo(\n",
    "        repo_id=repo_id,\n",
    "        token=token,\n",
    "        private=False,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        tempdirname = Path(tmpdirname)\n",
    "\n",
    "        # Step 2: Save the model\n",
    "        torch.save(model.state_dict(), tempdirname / \"model.pt\")\n",
    "\n",
    "        # Step 3: Evaluate the model and build JSON\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env,\n",
    "                                                  10,\n",
    "                                                  model)\n",
    "        # First get datetime\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        # Then convert to string\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        # Build JSON\n",
    "        evaluate_data = {\n",
    "            \"env_id\": hyperparameters.env_id,\n",
    "            \"mean_reward\": mean_reward,\n",
    "            \"std_reward\": std_reward,\n",
    "            \"n_eval_episodes\": 10,\n",
    "            \"eval_datetime\": eval_form_datetime,\n",
    "        }\n",
    "\n",
    "        # Write a JSON file\n",
    "        with open(tmpdirname / \"results.json\", \"w\") as f:\n",
    "            json.dump(evaluate_data, f)\n",
    "\n",
    "        # Step 4: Generate a video\n",
    "        video_path = tmpdirname / \"replay.mp4\"\n",
    "        record_video(eval_env, model, video_path, video_fps=video_fps)\n",
    "\n",
    "        # Step 5: Generate a model card\n",
    "        generated_model_card, metadata = _generate_model_card(\"PPO\",\n",
    "                                                                hyperparameters.env_id,\n",
    "                                                                mean_reward,\n",
    "                                                                std_reward,\n",
    "                                                                hyperparameters)\n",
    "        _save_model_card(tmpdirname, generated_model_card, metadata)\n",
    "\n",
    "        # Step 6: Add logs if needed\n",
    "        if logs:\n",
    "            _add_logdir(tmpdirname, Path(logs))\n",
    "\n",
    "        msg.info(f\"Pushing the model to the hub under the id {repo_id}...\")\n",
    "\n",
    "        repo_url = upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=tmpdirname,\n",
    "            path_in_repo=\"\",\n",
    "            commit_message=commit_message,\n",
    "            token=token,\n",
    "        )\n",
    "        \n",
    "        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
    "        return repo_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding HuggingFace argument\n",
    "import argparse\n",
    "import os\n",
    "from distutils.util import strtobool\n",
    "\n",
    "def parse_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp-name\", type=str, default=os.path.abspath('').rstrip(\".py\"), \n",
    "                        help=\"Name of the experiment\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for reproducibility\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True, help=\"If Toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "    \n",
    "    # Alogrithm specific arguments\n",
    "    parser.add_argument(\"--env-id\", type=str, default=\"LunarLander-v2\",\n",
    "        help=\"the id of the environment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=20000, # should set it to 1_000_000 when running on your own\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--num-envs\", type=int, default=16,\n",
    "        help=\"the number of parallel game environments\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=1024,\n",
    "        help=\"the number of steps to run in each environment per policy rollout\")\n",
    "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
    "    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Use GAE for advantage computation\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.999,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.98,\n",
    "        help=\"the lambda for the general advantage estimation\")\n",
    "    parser.add_argument(\"--num-minibatches\", type=int, default=64,\n",
    "        help=\"the number of mini-batches\")\n",
    "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n",
    "        help=\"the K epochs to update the policy\")\n",
    "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles advantages normalization\")\n",
    "    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
    "        help=\"the surrogate clipping coefficient\")\n",
    "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
    "        help=\"coefficient of the entropy\")\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
    "        help=\"coefficient of the value function\")\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "        help=\"the maximum norm for the gradient clipping\")\n",
    "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
    "        help=\"the target KL divergence threshold\")\n",
    "\n",
    "    # Adding HuggingFace argument    \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--repo-id\",\n",
    "        type=str,\n",
    "        default=\"RajkNakka/ppo-LunarLander-v2-unit-8\",\n",
    "        help=\"id of the model repository from the Hugging Face Hbu {username/repo-name}\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--exp-name EXP_NAME] [--seed SEED]\n",
      "                             [--torch-deterministic [TORCH_DETERMINISTIC]]\n",
      "                             [--cuda [CUDA]] [--track [TRACK]]\n",
      "                             [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "                             [--wandb-entity WANDB_ENTITY]\n",
      "                             [--capture-video [CAPTURE_VIDEO]]\n",
      "                             [--env-id ENV_ID]\n",
      "                             [--total-timesteps TOTAL_TIMESTEPS]\n",
      "                             [--learning-rate LEARNING_RATE]\n",
      "                             [--num-envs NUM_ENVS] [--num-steps NUM_STEPS]\n",
      "                             [--anneal-lr [ANNEAL_LR]] [--gae [GAE]]\n",
      "                             [--gamma GAMMA] [--gae-lambda GAE_LAMBDA]\n",
      "                             [--num-minibatches NUM_MINIBATCHES]\n",
      "                             [--update-epochs UPDATE_EPOCHS]\n",
      "                             [--norm-adv [NORM_ADV]] [--clip-coef CLIP_COEF]\n",
      "                             [--clip-vloss [CLIP_VLOSS]] [--ent-coef ENT_COEF]\n",
      "                             [--vf-coef VF_COEF]\n",
      "                             [--max-grad-norm MAX_GRAD_NORM]\n",
      "                             [--target-kl TARGET_KL] [--repo-id REPO_ID]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"66d6f075-2107-40db-bc18-3626733b4565\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/home/raj/.local/share/jupyter/runtime/kernel-v2-185041ScJbTIPqdtS.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/repos/HF-DeepRL/8-Proximal-Policy-Optimization/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# The main function\n",
    "# Imports needed for functions defined below\n",
    "\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tempfile\n",
    "import json\n",
    "import shutil\n",
    "import imageio_ffmpeg\n",
    "import random\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions.categorical as Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # if running from command line use this\n",
    "    # args = parse_args()\n",
    "    \n",
    "    # to run in jupyter notebook use this\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--exp-name\", type=str, default=os.path.abspath('').rstrip(\".py\"))\n",
    "    # parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for reproducibility\")\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparams\", \n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{k}|{v}|\" for k, v in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # Try not to modify: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, i, args.seed +i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    # Note how `next_obs` and `next_done` are used; their usage is equivalent to `obs[1:]` and `dones[1:]`\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "    for update in range(1, num_updates+1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = args.learning_rate * frac\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1 * args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: put action logic here\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.Tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, episode_reward={item['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episode_reward\", item['episode']['r'], global_step)\n",
    "                    writer.add_scalar(\"charts/episode_length\", item['episode']['l'], global_step)\n",
    "                    break\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            if args.gae:\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t+1]\n",
    "                        nextvalues = values[t+1]\n",
    "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards).to(device)\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t+1]\n",
    "                        next_return = returns[t+1]\n",
    "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,)+envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,)+envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizaing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for i_epoch_pi in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalues = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl divergence using logratio\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approxkl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                # calculate advantages\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # calculate policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1.0 - args.clip_coef, 1.0 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # calculate value loss using either the clipped value or MSE\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(newvalues - b_values[mb_inds], -args.clip_coef, args.clip_coef)\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                # calculate entropy loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                if args.target_kl is not None and approxkl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "            y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "            var_y = np.var(y_true)\n",
    "            explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "            # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "            writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "            writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "            writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "            writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "            writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "            print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        envs.close()\n",
    "        writer.close()\n",
    "\n",
    "        # Create the evaluation environment\n",
    "        # eval_env = gym.make(args.env_id)\n",
    "\n",
    "        # package_to_hub(repo_id = args.repo_id,\n",
    "        #             model = agent, # The model we want to save\n",
    "        #             hyperparameters = args,\n",
    "        #             eval_env = gym.make(args.env_id),\n",
    "        #             logs= f\"runs/{run_name}\",\n",
    "        #             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbc14ca4fb84a6dba389d175e365bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login to huggingface from notebook\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "global_step=1008, episodic_return=-111.396484375\n",
      "global_step=1024, episodic_return=-249.04827880859375\n",
      "global_step=1104, episodic_return=-164.42489624023438\n",
      "global_step=1280, episodic_return=-356.3940734863281\n",
      "global_step=1392, episodic_return=-260.03045654296875\n",
      "global_step=1408, episodic_return=-130.0479736328125\n",
      "global_step=1488, episodic_return=-78.94027709960938\n",
      "global_step=1536, episodic_return=-237.42181396484375\n",
      "global_step=1552, episodic_return=-209.61349487304688\n",
      "global_step=1600, episodic_return=-208.95553588867188\n",
      "global_step=1760, episodic_return=-364.6192321777344\n",
      "global_step=1904, episodic_return=-113.28790283203125\n",
      "global_step=2016, episodic_return=-107.48539733886719\n",
      "global_step=2064, episodic_return=-143.09176635742188\n",
      "global_step=2288, episodic_return=-77.35099029541016\n",
      "global_step=2352, episodic_return=-134.1401824951172\n",
      "global_step=2448, episodic_return=-193.4730224609375\n",
      "global_step=2512, episodic_return=-86.79818725585938\n",
      "global_step=2672, episodic_return=-79.32128143310547\n",
      "global_step=2720, episodic_return=-220.4950408935547\n",
      "global_step=2784, episodic_return=-155.39346313476562\n",
      "global_step=2992, episodic_return=-88.6456069946289\n",
      "global_step=3088, episodic_return=-251.55923461914062\n",
      "global_step=3104, episodic_return=-389.76611328125\n",
      "global_step=3232, episodic_return=-103.86853790283203\n",
      "global_step=3248, episodic_return=-302.830810546875\n",
      "global_step=3376, episodic_return=-40.18428039550781\n",
      "global_step=3536, episodic_return=-217.97998046875\n",
      "global_step=3584, episodic_return=-198.91114807128906\n",
      "global_step=3664, episodic_return=-210.38595581054688\n",
      "global_step=3696, episodic_return=-101.4321060180664\n",
      "global_step=4016, episodic_return=-362.15655517578125\n",
      "global_step=4144, episodic_return=-141.87051391601562\n",
      "global_step=4272, episodic_return=-391.6820373535156\n",
      "global_step=4432, episodic_return=-110.74937438964844\n",
      "global_step=4496, episodic_return=-14.814674377441406\n",
      "global_step=4512, episodic_return=-120.86821746826172\n",
      "global_step=4720, episodic_return=-299.549560546875\n",
      "global_step=4832, episodic_return=-137.70098876953125\n",
      "global_step=4896, episodic_return=-142.00450134277344\n",
      "global_step=4928, episodic_return=-167.80276489257812\n",
      "global_step=5088, episodic_return=-286.0780334472656\n",
      "global_step=5296, episodic_return=-14.972465515136719\n",
      "global_step=5328, episodic_return=-384.7533264160156\n",
      "global_step=5344, episodic_return=-232.24818420410156\n",
      "global_step=5360, episodic_return=-197.23165893554688\n",
      "global_step=5424, episodic_return=-120.71609497070312\n",
      "global_step=5488, episodic_return=-95.73426818847656\n",
      "global_step=5744, episodic_return=-118.92387390136719\n",
      "global_step=5824, episodic_return=-329.9462890625\n",
      "global_step=5888, episodic_return=-382.39556884765625\n",
      "global_step=5904, episodic_return=-216.42745971679688\n",
      "global_step=6048, episodic_return=-134.4536590576172\n",
      "global_step=6096, episodic_return=-86.56375885009766\n",
      "global_step=6352, episodic_return=-270.49951171875\n",
      "global_step=6400, episodic_return=-93.32855987548828\n",
      "global_step=6560, episodic_return=-171.86172485351562\n",
      "global_step=6720, episodic_return=-120.10308837890625\n",
      "global_step=6752, episodic_return=-84.94172668457031\n",
      "global_step=6816, episodic_return=-123.17805480957031\n",
      "global_step=6832, episodic_return=-511.2051696777344\n",
      "global_step=6976, episodic_return=-102.79136657714844\n",
      "global_step=7152, episodic_return=-364.90478515625\n",
      "global_step=7312, episodic_return=-383.0530700683594\n",
      "global_step=7376, episodic_return=-293.8866882324219\n",
      "global_step=7504, episodic_return=-83.44898986816406\n",
      "global_step=7568, episodic_return=-351.53369140625\n",
      "global_step=7584, episodic_return=-117.1436767578125\n",
      "global_step=7616, episodic_return=-333.3149719238281\n",
      "global_step=7744, episodic_return=-77.25328826904297\n",
      "global_step=7760, episodic_return=-65.22509002685547\n",
      "global_step=7968, episodic_return=-59.94446563720703\n",
      "global_step=8032, episodic_return=-222.466064453125\n",
      "global_step=8048, episodic_return=-167.82814025878906\n",
      "global_step=8144, episodic_return=-125.23534393310547\n",
      "global_step=8432, episodic_return=-58.28432083129883\n",
      "global_step=8512, episodic_return=-362.4178466796875\n",
      "global_step=8576, episodic_return=-349.3759765625\n",
      "global_step=8752, episodic_return=-147.21827697753906\n",
      "global_step=8784, episodic_return=-75.4028091430664\n",
      "global_step=8800, episodic_return=-275.7203369140625\n",
      "global_step=8976, episodic_return=-96.66913604736328\n",
      "global_step=8992, episodic_return=-220.46112060546875\n",
      "global_step=9104, episodic_return=-126.83148193359375\n",
      "global_step=9136, episodic_return=-154.80288696289062\n",
      "global_step=9184, episodic_return=-162.41552734375\n",
      "global_step=9200, episodic_return=-103.73817443847656\n",
      "global_step=9248, episodic_return=-119.70565795898438\n",
      "global_step=9456, episodic_return=-162.36825561523438\n",
      "global_step=9472, episodic_return=-118.40365600585938\n",
      "global_step=9488, episodic_return=-63.7220458984375\n",
      "global_step=9568, episodic_return=-156.0911102294922\n",
      "global_step=9952, episodic_return=-98.63867950439453\n",
      "global_step=9968, episodic_return=-86.99537658691406\n",
      "global_step=10000, episodic_return=-154.23817443847656\n",
      "global_step=10096, episodic_return=-80.77848052978516\n",
      "global_step=10144, episodic_return=-95.42520141601562\n",
      "global_step=10336, episodic_return=-389.0301208496094\n",
      "global_step=10416, episodic_return=-495.85430908203125\n",
      "global_step=10528, episodic_return=-277.2227783203125\n",
      "global_step=10640, episodic_return=-320.74249267578125\n",
      "global_step=10656, episodic_return=-154.4646453857422\n",
      "global_step=10784, episodic_return=-74.6021499633789\n",
      "global_step=10816, episodic_return=-170.91921997070312\n",
      "global_step=10944, episodic_return=-179.86038208007812\n",
      "global_step=10976, episodic_return=-119.3814697265625\n",
      "global_step=11008, episodic_return=-141.69561767578125\n",
      "global_step=11120, episodic_return=-121.20137023925781\n",
      "global_step=11168, episodic_return=-268.9571533203125\n",
      "global_step=11392, episodic_return=-107.73031616210938\n",
      "global_step=11536, episodic_return=-109.488525390625\n",
      "global_step=11680, episodic_return=-137.28321838378906\n",
      "global_step=12000, episodic_return=-134.39306640625\n",
      "global_step=12016, episodic_return=-101.0047607421875\n",
      "global_step=12048, episodic_return=-200.98883056640625\n",
      "global_step=12176, episodic_return=-242.3367462158203\n",
      "global_step=12272, episodic_return=-402.1939392089844\n",
      "global_step=12368, episodic_return=-4.03369140625\n",
      "global_step=12448, episodic_return=-108.13026428222656\n",
      "global_step=12480, episodic_return=-148.28936767578125\n",
      "global_step=12528, episodic_return=-72.7730712890625\n",
      "global_step=12960, episodic_return=-1.56414794921875\n",
      "global_step=13024, episodic_return=-123.92225646972656\n",
      "global_step=13120, episodic_return=-67.72289276123047\n",
      "global_step=13184, episodic_return=-154.56405639648438\n",
      "global_step=13216, episodic_return=-105.58403015136719\n",
      "global_step=13232, episodic_return=-421.4708251953125\n",
      "global_step=13248, episodic_return=-91.60735321044922\n",
      "global_step=13520, episodic_return=-431.22125244140625\n",
      "global_step=13552, episodic_return=-127.06881713867188\n",
      "global_step=13568, episodic_return=-213.32394409179688\n",
      "global_step=13776, episodic_return=15.321052551269531\n",
      "global_step=13936, episodic_return=-180.63941955566406\n",
      "global_step=14112, episodic_return=-105.35454559326172\n",
      "global_step=14272, episodic_return=-103.06401824951172\n",
      "global_step=14304, episodic_return=-112.79035949707031\n",
      "global_step=14320, episodic_return=-85.07978820800781\n",
      "global_step=14384, episodic_return=-105.21037292480469\n",
      "global_step=14400, episodic_return=-121.45353698730469\n",
      "global_step=14432, episodic_return=-177.74356079101562\n",
      "global_step=14512, episodic_return=-128.47265625\n",
      "global_step=14608, episodic_return=-21.255104064941406\n",
      "global_step=14736, episodic_return=-418.80859375\n",
      "global_step=14896, episodic_return=-265.5834655761719\n",
      "global_step=14912, episodic_return=-274.7816162109375\n",
      "global_step=15104, episodic_return=-288.1709899902344\n",
      "global_step=15456, episodic_return=-88.64512634277344\n",
      "global_step=15552, episodic_return=-87.13609313964844\n",
      "global_step=15664, episodic_return=-96.52069091796875\n",
      "global_step=15728, episodic_return=-282.6339111328125\n",
      "global_step=15760, episodic_return=-351.4833984375\n",
      "global_step=15888, episodic_return=-418.00836181640625\n",
      "global_step=15968, episodic_return=-308.12677001953125\n",
      "global_step=16080, episodic_return=-341.61395263671875\n",
      "global_step=16112, episodic_return=-70.75979614257812\n",
      "global_step=16240, episodic_return=-204.78660583496094\n",
      "global_step=16288, episodic_return=-90.93756103515625\n",
      "global_step=16384, episodic_return=-417.0933837890625\n",
      "SPS: 3641\n"
     ]
    }
   ],
   "source": [
    "# Train the model without pushing to the hub: make sure the package_to_hub function is commented out\n",
    "!python ppo.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPS: 90\n",
      "\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n",
      "libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: swrast\n",
      "X Error of failed request:  BadValue (integer parameter out of range for operation)\n",
      "  Major opcode of failed request:  150 (GLX)\n",
      "  Minor opcode of failed request:  3 (X_GLXCreateContext)\n",
      "  Value in failed request:  0x0\n",
      "  Serial number of failed request:  92\n",
      "  Current serial number in output stream:  93\n"
     ]
    }
   ],
   "source": [
    "# Train, evaluate and push to the hub\n",
    "!python ppo.py --total-timesteps 100 --capture-video False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2847039533.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[46], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    package_to_hub(repo_id = args.repo_id = \"RajkNakka/ppo-LunarLander-v2\",\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Push the model to the hub\n",
    "package_to_hub(repo_id = args.repo_id = \"RajkNakka/ppo-LunarLander-v2-unit-8\",\n",
    "            model = agent, # The model we want to save\n",
    "            hyperparameters = args,\n",
    "            eval_env = gym.make(args.env_id),\n",
    "            logs= f\"runs/{run_name}\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
